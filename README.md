Text processing tool, offering tokenization targeting text from social networks (twitter, facebook...), spell correction and word segmentation using word statistics from 2 different corpora (english Wikipedia, twitter - 330mil tweets)
###Tokenization
The tokenizer understands many expressions found in natural language such as dates, times, currencies, hashtags, emoticons and much more...
###Spell Correction
The Spell Corrector extends the functionality of Peter Norvig's spell-corrector.
###Word Segmentation
Word Segmentation that implements the Viterbi algorithm for word segmentation. Based on CH14 from the book Beautiful Data (Segaran and Hammerbacher, 2009)


documentation and examples coming soon...